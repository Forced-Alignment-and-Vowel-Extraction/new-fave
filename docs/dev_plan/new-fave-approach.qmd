---
title: "`new-fave` Approach"
date-modified: today
fig-height: 3
fig-width: 5
order: 1
---

Currently, I have a few classes and functions written up to allow for a 
hybrid fave-like and fasttrack-like formant track optimization.

```{python}
## key imports
from fasttrackpy import process_corpus
from new_fave.measurements.vowel_measurement import VowelClassCollection, VowelMeasurement, VowelClass
from new_fave.optimize.optimize import optimize_vowel_measures, optimize_one_measure

## support
import matplotlib.pyplot as plt
import numpy as np
import polars as pl
import re
from pathlib import Path
```

## The fasttrack step

The first step will be to use fasttrack to process all of the vowels in a corpus.

```{python}
corpus_path = Path("assets", "corpus")
list(corpus_path.glob("*"))
```

I'll optimize for 3 formants, with 100 steps between 4800 and 7300 max formant.

```{python}
#| output: false
candidates = process_corpus(
    corpus_path,
    entry_classes=["Word", "Phone"],
    min_max_formant=4800,
    max_max_formant=7300,
    n_formants=3,
    nstep=100
)
```

Quickly, let's look at our formants.

```{python}
first_pass = pl.concat([
    track.winner.to_df() for track in candidates
    ])
```

Normalizing time.
```{python}
#| code-fold: true
#| code-summary: "time normalizing"
first_pass = (
    first_pass
    .with_columns(
        (pl.col("time")/(
            pl.col("time").max() + pl.col("time").min()
            )
        ).over("id").alias("prop_time")
    )
)
```

Here are the smoothed F1 and F2 tracks.

```{python}
#| code-fold: true
plt.scatter(
    first_pass["prop_time"],
    first_pass["F1_s"],
    s = 1
)
plt.title("Smoothed F1")
plt.show()
```

```{python}
#| code-fold: true
plt.scatter(
    first_pass["prop_time"],
    first_pass["F2_s"],
    s = 1
)
plt.title("Smoothed F2")
plt.show()
```

And in F1xF2 space

```{python}
#| code-fold: true
plt.scatter(
    -first_pass["F2_s"],
    -first_pass["F1_s"],
    s = 1
)
plt.title("F1xF2 space")
plt.show()
```

I'll grab the outliers based on a visual inspection. (F1 > 800 or F2 > 2500).

```{python}
#| code-fold: true
outliers = (
    first_pass
    .filter(
        (
            (pl.col("label").str.contains("1"))
            &
            (
                (pl.col("F1") > 900 ) |
                (pl.col("F2") > 2500)
            )
        )
    )
    .group_by(["id", "label"])
    .count()
)

outliers
```

And I'll look at their spectrograms
```{python}
#| code-fold: true
#| layout-ncol: 3
#| out-width: 100%
outlier_tracks = [
    track
    for track in candidates
    if track.id in outliers["id"]
]

for track in outlier_tracks:
    track.winner.spectrogram(figsize = (3,2))
    plt.title(track.label +" " +track.id)
    plt.show()
```


Some vowels may be lost causes, looking like instances of alignment errors. But the others may be salvageable, and seem to be due to their F1 and F2 being very close.

## Using the new-fave classes

The new-fave classes create vowel measurements, and vowel *classes* based on the labels in the candidate tracks. The final product will have a fave-recode step in here, but meanwhile I'll adjust the labels manually.

```{python}
for track in candidates:
    # schwa
    if track.label == "AH0":
        track.label = "@"
    # stress folding
    else:
        track.label = re.sub(r"\d", "", track.label)
```

These two steps are necessary to relate each vowel measurement to its vowel class.
More on `param_optim` later.

```{python}
v_meases = [VowelMeasurement(track) for track in candidates]
v_classes = VowelClassCollection(v_meases, param_optim=5)
```

The first vowel measurement is an `AY`, and the "winning" formant tracks in its vowel class are available.

```{python}
print(v_meases[0].label)
v_meases[0].vowel_class.winners
```

## Optimizing dimensions
There are three key dimensions that I can see trying to optimize across. 

1. The shape and value of the formant tracks.
2. The typical maximum-formant for the vowel class.
3. The smoothing error of each candidate track within a vowel measurement.

### The shape and value of the formant tracks

The shape and value of a single formant track is captured by the discrete cosine coefficients returned by fasttrackpy.
```{python}
#| echo: false
np.set_printoptions(suppress=True) 
```
```{python}
v_meases[0].winner.parameters
```

We could use the distribution of values across these parameters for FAVE-like optimization, e.g., to get the mahalanobis distance of each possible track's DCT coefficients from the vowel class average. The question is, how *many* of the DCT coefficients should we use?

In some sense, the first DCT coefficient is the most important, placing F1, F2 and F3 in a general position in formant space.

```{python}
#| code-fold: true

def plot_params(
    vm: VowelMeasurement,
    up_to = 5
):
    params = vm.vowel_class.winner_params.mean(axis = 2).T
    params[:,up_to:] = 0
    time = vm.winner.time_domain
    N = vm.winner.time_domain.size
    basis = np.array (
        [(np.cos(np.pi * (np.arange(N)/N) * k)) 
         for k in range(5)]
        )
    fit = params @ basis
    plt.plot(
        time,
        fit.T
    )
    plt.title(f"{up_to} params")
    plt.show()

```

```{python}
#| layout-ncol: 2
#| out-width: 50%

plot_params(v_meases[0], up_to = 1)
```

However, we probably also want the *typical* formant shape and value to also incorporate some dynamics.

```{python}
#| layout-ncol: 2
#| out-width: 50%

plot_params(v_meases[0], up_to = 2)
```

```{python}
#| layout-ncol: 2
#| out-width: 50%

plot_params(v_meases[0], up_to = 3)
```

While increasing the number of parameters is important for smoothing the *formant tracks*, in terms of describing, the typical trajectory, I have some concerns that it could wind up penalizing some candidate tracks for not being wiggly *enough* if their 4th or 5th parameters are too distant from the average.

#### Implementation
The way it's currently implemented, I have to specify how many formant tracks I'm optimizing over when I create the VowelClassCollection object.

```{python}
v_classes = VowelClassCollection(v_meases, param_optim=3)
```

I can then get the mahalanobis distance from each candidate to the vowel class.

```{python}
plt.scatter(
    v_meases[0].cand_max_formants[0],
    v_meases[0].cand_mahals
)
```

I've also implemented conversion of the mahalanobis distance to log-probabilities based on the chi-squared distribution.
```{python}
plt.scatter(
    v_meases[0].cand_max_formants[0],
    v_meases[0].cand_mahal_log_prob
)
```


### Typical Max Formants

Each vowel class will also have a typical max formant distribution.

```{python}
plt.hist(
    v_classes["AY"].winners_maximum_formant[0],
    bins = 10
)
plt.show()
```

We can also get a 1-dimensional mahalanobis distance from each candidate track to the typical maximum formant.

```{python}
plt.scatter(
    v_meases[0].cand_max_formants[0],
    v_meases[0].max_formant_mahal
)
```

This also has a log-probabilty definition.

```{python}
plt.scatter(
    v_meases[0].cand_max_formants[0],
    v_meases[0].max_formant_log_prob
)
```

### Interim synthesis

With these two log-probabilities, one way we could decide to choose a new optimal winner would be to take the argmax of the sum.

```{python}
plt.scatter(
    v_meases[0].cand_max_formants[0],
    v_meases[0].cand_mahal_log_prob +
        v_meases[0].max_formant_log_prob
)
```

```{python}
( 
    v_meases[0].cand_mahal_log_prob +
    v_meases[0].max_formant_log_prob
).argmax()
```

But this doesn't take into account the smoother error.

### Smoother error

Each candidate formant track also has a mean squared error value associated.

```{python}
plt.scatter(
    v_meases[0].cand_max_formants[0],
    v_meases[0].cand_errors
)
```

I wasn't sure how to associate these error terms to a probability distribution in order to convert them to a log-probability, so instead I calculated an empirical density function across the errors, and got the log probability from that.

```{python}
plt.scatter(
    v_meases[0].cand_max_formants[0],
    v_meases[0].error_log_prob
)
```

### Final Joint Probability

```{python}
plt.scatter(
    v_meases[0].cand_max_formants[0],
    v_meases[0].cand_mahal_log_prob +
        v_meases[0].max_formant_log_prob +
        v_meases[0].error_log_prob
)
```

```{python}
new_winner = ( 
    v_meases[0].cand_mahal_log_prob +
    v_meases[0].max_formant_log_prob +
    v_meases[0].error_log_prob
).argmax()
```


```{python}
orig_winner = v_meases[0].track.winner_idx
```


```{python}
v_meases[0].cand_max_formants[0][orig_winner]
```

```{python}
v_meases[0].cand_max_formants[0][new_winner]
```

This difference in maximum formant doesn't ahve a huge effect on the resulting data.

```{python}
orig_df = v_meases[0].candidates[orig_winner].to_df()
new_df = v_meases[0].candidates[orig_winner].to_df()
```

```{python}
plt.scatter(
    -orig_df["F2"],
    -orig_df["F1"],
    color = "red",
    label = "original",
    alpha = 0.3
)
plt.scatter(
    -new_df["F2"],
    -new_df["F1"],
    color = "blue",
    label = "new",
    alpha = 0.3
)
plt.legend()
```


## Optimizing

Open questions are:

- Are all three metrics important, or functional, for the kind of error reduction we want?
- How many optimizing steps should we take?
- How many max-formant increments should we use?

Here, I'll optimize for 10 steps
```{python}
formant_optim = np.array([vm.winner.maximum_formant for vm in v_meases])
for i in range(10):
    optimize_vowel_measures(v_meases)
    new_max = np.array([vm.winner.maximum_formant for vm in v_meases])
    formant_optim = np.vstack([formant_optim, new_max])
```

This plot shows how any single track was updated over optimization steps.

```{python}
plt.plot(
    range(11),
    formant_optim
)
plt.show()
```

```{python}
new_df = pl.concat([
    vm.winner.to_df()
    for vm in v_meases
])
```

```{python}
plt.scatter(
    -new_df["F2_s"],
    -new_df["F1_s"],
    s = 1
)
plt.title("F1xF2 space")
plt.show()
```

```{python}
new_outliers = (
    new_df
    .filter(
        (
            (pl.col("label").str.contains("1"))
            &
            (
                (pl.col("F1_s") > 900 ) |
                (pl.col("F2_s") > 2500)
            )
        )
    )
    .group_by(["id", "label"])
    .count()
)

new_outliers
```

```{python}
#| code-fold: true
#| layout-ncol: 3
#| out-width: 100%
new_outlier_tracks = [
    vm
    for vm in v_meases
    if vm.winner.id in new_outliers["id"]
]

for track in new_outlier_tracks:
    track.winner.spectrogram(figsize = (3,2))
    plt.title(track.label +" " +track.winner.id)
    plt.show()
```

These remaining outliers look like just fundamentally hard cases.

```{python}

plt.scatter(
    -new_df["F2_s"],
    -new_df["F1_s"],
    color = "grey"
)

for track in v_classes["AY"].tracks:
    df = track.winner.to_df()
    plt.plot(
        -df["F2_s"],
        -df["F1_s"]
    )
```