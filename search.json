[
  {
    "objectID": "reference/extract.html",
    "href": "reference/extract.html",
    "title": "extract",
    "section": "",
    "text": "extract\n\n\n\n\n\nName\nDescription\n\n\n\n\nfave_extract\nsummary\n\n\n\n\n\nextract.fave_extract(corpus_path, fasttrack_config, labelset_parser, recode_rules, measurement_points, **kwargs)\nsummary",
    "crumbs": [
      "Extract",
      "extract"
    ]
  },
  {
    "objectID": "reference/extract.html#functions",
    "href": "reference/extract.html#functions",
    "title": "extract",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nfave_extract\nsummary\n\n\n\n\n\nextract.fave_extract(corpus_path, fasttrack_config, labelset_parser, recode_rules, measurement_points, **kwargs)\nsummary",
    "crumbs": [
      "Extract",
      "extract"
    ]
  },
  {
    "objectID": "dev_plan/index.html",
    "href": "dev_plan/index.html",
    "title": "Development Plan",
    "section": "",
    "text": "The plan is for new-fave to be more opinionated about the input data stucture. fasttrackpy is more general purpose and therefore has its own design approach.\n\n\n\nThe plan is for new-fave to bring together, under one tool\n\nfave-recodeing of input data, allowing for dialect, language, or research question specific recoding of alignment output\nCustomizable point measurement heuristics\nEnriched data output enabled by its opinionated approach to data input. (e.g. fave-syllabify)\n\n\n\n\nSee New-Fave Approach",
    "crumbs": [
      "Home",
      "Development Plan"
    ]
  },
  {
    "objectID": "dev_plan/index.html#what-is-favey-about-this",
    "href": "dev_plan/index.html#what-is-favey-about-this",
    "title": "Development Plan",
    "section": "",
    "text": "The plan is for new-fave to be more opinionated about the input data stucture. fasttrackpy is more general purpose and therefore has its own design approach.\n\n\n\nThe plan is for new-fave to bring together, under one tool\n\nfave-recodeing of input data, allowing for dialect, language, or research question specific recoding of alignment output\nCustomizable point measurement heuristics\nEnriched data output enabled by its opinionated approach to data input. (e.g. fave-syllabify)\n\n\n\n\nSee New-Fave Approach",
    "crumbs": [
      "Home",
      "Development Plan"
    ]
  },
  {
    "objectID": "dev_plan/new-fave-approach.html",
    "href": "dev_plan/new-fave-approach.html",
    "title": "new-fave Approach",
    "section": "",
    "text": "Currently, I have a few classes and functions written up to allow for a hybrid fave-like and fasttrack-like formant track optimization.\n## key imports\nfrom fasttrackpy import process_corpus\nfrom new_fave.measurements.vowel_measurement import VowelClassCollection, VowelMeasurement, VowelClass\nfrom new_fave.optimize.optimize import optimize_vowel_measures, optimize_one_measure\n\n## support\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport polars as pl\nimport re\nfrom pathlib import Path",
    "crumbs": [
      "Home",
      "Development Plan",
      "`new-fave` Approach"
    ]
  },
  {
    "objectID": "dev_plan/new-fave-approach.html#the-fasttrack-step",
    "href": "dev_plan/new-fave-approach.html#the-fasttrack-step",
    "title": "new-fave Approach",
    "section": "The fasttrack step",
    "text": "The fasttrack step\nThe first step will be to use fasttrack to process all of the vowels in a corpus.\n\ncorpus_path = Path(\"assets\", \"corpus\")\nlist(corpus_path.glob(\"*\"))\n\n[PosixPath('assets/corpus/josef-fruehwald_speaker.TextGrid'),\n PosixPath('assets/corpus/josef-fruehwald_speaker.wav')]\n\n\nI’ll optimize for 3 formants, with 100 steps between 4800 and 7300 max formant.\n\ncandidates = process_corpus(\n    corpus_path,\n    entry_classes=[\"Word\", \"Phone\"],\n    min_max_formant=4800,\n    max_max_formant=7300,\n    n_formants=3,\n    nstep=100\n)\n\nQuickly, let’s look at our formants.\n\nfirst_pass = pl.concat([\n    track.winner.to_df() for track in candidates\n    ])\n\nNormalizing time.\n\n\ntime normalizing\nfirst_pass = (\n    first_pass\n    .with_columns(\n        (pl.col(\"time\")/(\n            pl.col(\"time\").max() + pl.col(\"time\").min()\n            )\n        ).over(\"id\").alias(\"prop_time\")\n    )\n)\n\n\nHere are the smoothed F1 and F2 tracks.\n\n\nCode\nplt.scatter(\n    first_pass[\"prop_time\"],\n    first_pass[\"F1_s\"],\n    s = 1\n)\nplt.title(\"Smoothed F1\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.scatter(\n    first_pass[\"prop_time\"],\n    first_pass[\"F2_s\"],\n    s = 1\n)\nplt.title(\"Smoothed F2\")\nplt.show()\n\n\n\n\n\n\n\n\n\nAnd in F1xF2 space\n\n\nCode\nplt.scatter(\n    -first_pass[\"F2_s\"],\n    -first_pass[\"F1_s\"],\n    s = 1\n)\nplt.title(\"F1xF2 space\")\nplt.show()\n\n\n\n\n\n\n\n\n\nI’ll grab the outliers based on a visual inspection. (F1 &gt; 800 or F2 &gt; 2500).\n\n\nCode\noutliers = (\n    first_pass\n    .filter(\n        (\n            (pl.col(\"label\").str.contains(\"1\"))\n            &\n            (\n                (pl.col(\"F1\") &gt; 900 ) |\n                (pl.col(\"F2\") &gt; 2500)\n            )\n        )\n    )\n    .group_by([\"id\", \"label\"])\n    .count()\n)\n\noutliers\n\n\n/tmp/ipykernel_2113/1712949402.py:14: DeprecationWarning: `count` is deprecated. It has been renamed to `len`.\n  .count()\n\n\n\n\nshape: (5, 3)\n\n\n\nid\nlabel\ncount\n\n\nstr\nstr\nu32\n\n\n\n\n\"0-0-161-0\"\n\"AO1\"\n3\n\n\n\"0-0-314-1\"\n\"AO1\"\n83\n\n\n\"0-0-44-0\"\n\"AA1\"\n18\n\n\n\"0-0-371-0\"\n\"AO1\"\n19\n\n\n\"0-0-73-1\"\n\"OW1\"\n12\n\n\n\n\n\n\n\nAnd I’ll look at their spectrograms\n\nCode\noutlier_tracks = [\n    track\n    for track in candidates\n    if track.id in outliers[\"id\"]\n]\n\nfor track in outlier_tracks:\n    track.winner.spectrogram(figsize = (3,2))\n    plt.title(track.label +\" \" +track.id)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome vowels may be lost causes, looking like instances of alignment errors. But the others may be salvageable, and seem to be due to their F1 and F2 being very close.",
    "crumbs": [
      "Home",
      "Development Plan",
      "`new-fave` Approach"
    ]
  },
  {
    "objectID": "dev_plan/new-fave-approach.html#using-the-new-fave-classes",
    "href": "dev_plan/new-fave-approach.html#using-the-new-fave-classes",
    "title": "new-fave Approach",
    "section": "Using the new-fave classes",
    "text": "Using the new-fave classes\nThe new-fave classes create vowel measurements, and vowel classes based on the labels in the candidate tracks. The final product will have a fave-recode step in here, but meanwhile I’ll adjust the labels manually.\n\nfor track in candidates:\n    # schwa\n    if track.label == \"AH0\":\n        track.label = \"@\"\n    # stress folding\n    else:\n        track.label = re.sub(r\"\\d\", \"\", track.label)\n\nThese two steps are necessary to relate each vowel measurement to its vowel class. More on param_optim later.\n\nv_meases = [VowelMeasurement(track) for track in candidates]\nv_classes = VowelClassCollection(v_meases, param_optim=5)\n\nThe first vowel measurement is an AY, and the “winning” formant tracks in its vowel class are available.\n\nprint(v_meases[0].label)\nv_meases[0].vowel_class.winners\n\nAY\n\n\n[A formant track object. (3, 28),\n A formant track object. (3, 23),\n A formant track object. (3, 38),\n A formant track object. (3, 33),\n A formant track object. (3, 48),\n A formant track object. (3, 68),\n A formant track object. (3, 63),\n A formant track object. (3, 53),\n A formant track object. (3, 153),\n A formant track object. (3, 18),\n A formant track object. (3, 23),\n A formant track object. (3, 58),\n A formant track object. (3, 63),\n A formant track object. (3, 38),\n A formant track object. (3, 53),\n A formant track object. (3, 68),\n A formant track object. (3, 28),\n A formant track object. (3, 78),\n A formant track object. (3, 33),\n A formant track object. (3, 18)]",
    "crumbs": [
      "Home",
      "Development Plan",
      "`new-fave` Approach"
    ]
  },
  {
    "objectID": "dev_plan/new-fave-approach.html#optimizing-dimensions",
    "href": "dev_plan/new-fave-approach.html#optimizing-dimensions",
    "title": "new-fave Approach",
    "section": "Optimizing dimensions",
    "text": "Optimizing dimensions\nThere are three key dimensions that I can see trying to optimize across.\n\nThe shape and value of the formant tracks.\nThe typical maximum-formant for the vowel class.\nThe smoothing error of each candidate track within a vowel measurement.\n\n\nThe shape and value of the formant tracks\nThe shape and value of a single formant track is captured by the discrete cosine coefficients returned by fasttrackpy.\n\nv_meases[0].winner.parameters\n\narray([[ 462.68749031,   76.20382638,   10.52547247,    7.41706717,\n         -11.5939131 ],\n       [1671.60750571, -297.30469002,  -13.31639154,  -31.55773489,\n           1.121091  ],\n       [2431.9348866 ,  135.06260455,   90.83794963,    5.90733349,\n          -6.04934349]])\n\n\nWe could use the distribution of values across these parameters for FAVE-like optimization, e.g., to get the mahalanobis distance of each possible track’s DCT coefficients from the vowel class average. The question is, how many of the DCT coefficients should we use?\nIn some sense, the first DCT coefficient is the most important, placing F1, F2 and F3 in a general position in formant space.\n\n\nCode\ndef plot_params(\n    vm: VowelMeasurement,\n    up_to = 5\n):\n    params = vm.vowel_class.winner_params.mean(axis = 2).T\n    params[:,up_to:] = 0\n    time = vm.winner.time_domain\n    N = vm.winner.time_domain.size\n    basis = np.array (\n        [(np.cos(np.pi * (np.arange(N)/N) * k)) \n         for k in range(5)]\n        )\n    fit = params @ basis\n    plt.plot(\n        time,\n        fit.T\n    )\n    plt.title(f\"{up_to} params\")\n    plt.show()\n\n\nplot_params(v_meases[0], up_to = 1)\n\n\n\n\n\n\n\nHowever, we probably also want the typical formant shape and value to also incorporate some dynamics.\nplot_params(v_meases[0], up_to = 2)\n\n\n\n\n\n\n\nplot_params(v_meases[0], up_to = 3)\n\n\n\n\n\n\n\nWhile increasing the number of parameters is important for smoothing the formant tracks, in terms of describing, the typical trajectory, I have some concerns that it could wind up penalizing some candidate tracks for not being wiggly enough if their 4th or 5th parameters are too distant from the average.\n\nImplementation\nThe way it’s currently implemented, I have to specify how many formant tracks I’m optimizing over when I create the VowelClassCollection object.\n\nv_classes = VowelClassCollection(v_meases, param_optim=3)\n\nI can then get the mahalanobis distance from each candidate to the vowel class.\n\nplt.scatter(\n    v_meases[0].cand_max_formants[0],\n    v_meases[0].cand_mahals\n)\n\n\n\n\n\n\n\n\nI’ve also implemented conversion of the mahalanobis distance to log-probabilities based on the chi-squared distribution.\n\nplt.scatter(\n    v_meases[0].cand_max_formants[0],\n    v_meases[0].cand_mahal_log_prob\n)\n\n\n\n\n\n\n\n\n\n\n\nTypical Max Formants\nEach vowel class will also have a typical max formant distribution.\n\nplt.hist(\n    v_classes[\"AY\"].winners_maximum_formant[0],\n    bins = 10\n)\nplt.show()\n\n\n\n\n\n\n\n\nWe can also get a 1-dimensional mahalanobis distance from each candidate track to the typical maximum formant.\n\nplt.scatter(\n    v_meases[0].cand_max_formants[0],\n    v_meases[0].max_formant_mahal\n)\n\n\n\n\n\n\n\n\nThis also has a log-probabilty definition.\n\nplt.scatter(\n    v_meases[0].cand_max_formants[0],\n    v_meases[0].max_formant_log_prob\n)\n\n\n\n\n\n\n\n\n\n\nInterim synthesis\nWith these two log-probabilities, one way we could decide to choose a new optimal winner would be to take the argmax of the sum.\n\nplt.scatter(\n    v_meases[0].cand_max_formants[0],\n    v_meases[0].cand_mahal_log_prob +\n        v_meases[0].max_formant_log_prob\n)\n\n\n\n\n\n\n\n\n\n( \n    v_meases[0].cand_mahal_log_prob +\n    v_meases[0].max_formant_log_prob\n).argmax()\n\n35\n\n\nBut this doesn’t take into account the smoother error.\n\n\nSmoother error\nEach candidate formant track also has a mean squared error value associated.\n\nplt.scatter(\n    v_meases[0].cand_max_formants[0],\n    v_meases[0].cand_errors\n)\n\n\n\n\n\n\n\n\nI wasn’t sure how to associate these error terms to a probability distribution in order to convert them to a log-probability, so instead I calculated an empirical density function across the errors, and got the log probability from that.\n\nplt.scatter(\n    v_meases[0].cand_max_formants[0],\n    v_meases[0].error_log_prob\n)\n\n\n\n\n\n\n\n\n\n\nFinal Joint Probability\n\nplt.scatter(\n    v_meases[0].cand_max_formants[0],\n    v_meases[0].cand_mahal_log_prob +\n        v_meases[0].max_formant_log_prob +\n        v_meases[0].error_log_prob\n)\n\n\n\n\n\n\n\n\n\nnew_winner = ( \n    v_meases[0].cand_mahal_log_prob +\n    v_meases[0].max_formant_log_prob +\n    v_meases[0].error_log_prob\n).argmax()\n\n\norig_winner = v_meases[0].track.winner_idx\n\n\nv_meases[0].cand_max_formants[0][orig_winner]\n\n5229.292929292929\n\n\n\nv_meases[0].cand_max_formants[0][new_winner]\n\n5582.828282828283\n\n\nThis difference in maximum formant doesn’t ahve a huge effect on the resulting data.\n\norig_df = v_meases[0].candidates[orig_winner].to_df()\nnew_df = v_meases[0].candidates[orig_winner].to_df()\n\n\nplt.scatter(\n    -orig_df[\"F2\"],\n    -orig_df[\"F1\"],\n    color = \"red\",\n    label = \"original\",\n    alpha = 0.3\n)\nplt.scatter(\n    -new_df[\"F2\"],\n    -new_df[\"F1\"],\n    color = \"blue\",\n    label = \"new\",\n    alpha = 0.3\n)\nplt.legend()",
    "crumbs": [
      "Home",
      "Development Plan",
      "`new-fave` Approach"
    ]
  },
  {
    "objectID": "dev_plan/new-fave-approach.html#optimizing",
    "href": "dev_plan/new-fave-approach.html#optimizing",
    "title": "new-fave Approach",
    "section": "Optimizing",
    "text": "Optimizing\nOpen questions are:\n\nAre all three metrics important, or functional, for the kind of error reduction we want?\nHow many optimizing steps should we take?\nHow many max-formant increments should we use?\n\nHere, I’ll optimize for 10 steps\n\nformant_optim = np.array([vm.winner.maximum_formant for vm in v_meases])\nfor i in range(10):\n    optimize_vowel_measures(v_meases)\n    new_max = np.array([vm.winner.maximum_formant for vm in v_meases])\n    formant_optim = np.vstack([formant_optim, new_max])\n\nThis plot shows how any single track was updated over optimization steps.\n\nplt.plot(\n    range(11),\n    formant_optim\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nnew_df = pl.concat([\n    vm.winner.to_df()\n    for vm in v_meases\n])\n\n\nplt.scatter(\n    -new_df[\"F2_s\"],\n    -new_df[\"F1_s\"],\n    s = 1\n)\nplt.title(\"F1xF2 space\")\nplt.show()\n\n\n\n\n\n\n\n\n\nnew_outliers = (\n    new_df\n    .filter(\n        (\n            (pl.col(\"label\").str.contains(\"1\"))\n            &\n            (\n                (pl.col(\"F1_s\") &gt; 900 ) |\n                (pl.col(\"F2_s\") &gt; 2500)\n            )\n        )\n    )\n    .group_by([\"id\", \"label\"])\n    .count()\n)\n\nnew_outliers\n\n/tmp/ipykernel_2113/1290466927.py:14: DeprecationWarning: `count` is deprecated. It has been renamed to `len`.\n  .count()\n\n\n\n\nshape: (4, 3)\n\n\n\nid\nlabel\ncount\n\n\nstr\nstr\nu32\n\n\n\n\n\"0-0-73-1\"\n\"OW1\"\n11\n\n\n\"0-0-85-1\"\n\"AY1\"\n23\n\n\n\"0-0-44-0\"\n\"AA1\"\n8\n\n\n\"0-0-314-1\"\n\"AO1\"\n83\n\n\n\n\n\n\n\n\nCode\nnew_outlier_tracks = [\n    vm\n    for vm in v_meases\n    if vm.winner.id in new_outliers[\"id\"]\n]\n\nfor track in new_outlier_tracks:\n    track.winner.spectrogram(figsize = (3,2))\n    plt.title(track.label +\" \" +track.winner.id)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese remaining outliers look like just fundamentally hard cases.\n\nplt.scatter(\n    -new_df[\"F2_s\"],\n    -new_df[\"F1_s\"],\n    color = \"grey\"\n)\n\nfor track in v_classes[\"AY\"].tracks:\n    df = track.winner.to_df()\n    plt.plot(\n        -df[\"F2_s\"],\n        -df[\"F1_s\"]\n    )",
    "crumbs": [
      "Home",
      "Development Plan",
      "`new-fave` Approach"
    ]
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Function reference",
    "section": "",
    "text": "Extract\n\n\n\nextract",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#extract",
    "href": "reference/index.html#extract",
    "title": "Function reference",
    "section": "",
    "text": "Extract\n\n\n\nextract",
    "crumbs": [
      "Function reference"
    ]
  }
]