[
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Function reference",
    "section": "",
    "text": "Extract\n\n\n\nextract",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#extract",
    "href": "reference/index.html#extract",
    "title": "Function reference",
    "section": "",
    "text": "Extract\n\n\n\nextract",
    "crumbs": [
      "Function reference"
    ]
  },
  {
    "objectID": "dev_plan/index.html",
    "href": "dev_plan/index.html",
    "title": "Development Plan",
    "section": "",
    "text": "The plan is for new-fave to be more opinionated about the input data stucture. fasttrackpy is more general purpose and therefore has its own design approach.\n\n\n\nThe plan is for new-fave to bring together, under one tool\n\nfave-recodeing of input data, allowing for dialect, language, or research question specific recoding of alignment output\nCustomizable point measurement heuristics\nEnriched data output enabled by its opinionated approach to data input. (e.g. fave-syllabify)\n\n\n\n\nSee New-Fave Approach",
    "crumbs": [
      "Home",
      "Development Plan"
    ]
  },
  {
    "objectID": "dev_plan/index.html#what-is-favey-about-this",
    "href": "dev_plan/index.html#what-is-favey-about-this",
    "title": "Development Plan",
    "section": "",
    "text": "The plan is for new-fave to be more opinionated about the input data stucture. fasttrackpy is more general purpose and therefore has its own design approach.\n\n\n\nThe plan is for new-fave to bring together, under one tool\n\nfave-recodeing of input data, allowing for dialect, language, or research question specific recoding of alignment output\nCustomizable point measurement heuristics\nEnriched data output enabled by its opinionated approach to data input. (e.g. fave-syllabify)\n\n\n\n\nSee New-Fave Approach",
    "crumbs": [
      "Home",
      "Development Plan"
    ]
  },
  {
    "objectID": "dev_plan/new-fave-approach.html",
    "href": "dev_plan/new-fave-approach.html",
    "title": "new-fave Approach",
    "section": "",
    "text": "Each vowel measurement from fasttrackpy has two possible output formats:\nThe plan is to use the smoothing parameters in much the same way that original FAVE used the reference values for F1, F2, bandwidth F1 and bandwidth F2.",
    "crumbs": [
      "Home",
      "Development Plan",
      "`new-fave` Approach"
    ]
  },
  {
    "objectID": "dev_plan/new-fave-approach.html#conceptual-demonstration",
    "href": "dev_plan/new-fave-approach.html#conceptual-demonstration",
    "title": "new-fave Approach",
    "section": "Conceptual Demonstration",
    "text": "Conceptual Demonstration\n\nimport IPython\nfrom fasttrackpy import process_audio_file, \\\n    process_directory, \\\n    process_audio_textgrid,\\\n    process_corpus\nimport polars as pl\nimport numpy as np\nfrom pathlib import Path\n\nWe can run fasttrackpy on the audio file ay.wav\n\naudio_path = Path(\"assets\", \"ay.wav\")\nIPython.display.Audio(audio_path)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\ncandidates = process_audio_file(\n    path=audio_path, \n    nstep = 100,\n    min_max_formant=3000,\n    max_max_formant=6000\n    )\n\nFrom the candidates object, we can get a polars dataframe of the discrete cosine parameters for\n\nAll candidates\nThe winner candidate\n\n\nwinner_df = candidates.to_df(\n    which = \"winner\", \n    output = \"param\"\n    )\nwinner_df\n\n\n\nshape: (5, 6)\n\n\n\nF1\nF2\nF3\nF4\nerror\nfile_name\n\n\nf64\nf64\nf64\nf64\nf64\nstr\n\n\n\n\n543.719318\n1648.995798\n2460.113393\n3263.261512\n0.026419\n\"ay.wav\"\n\n\n166.338746\n-551.8309\n34.624759\n-234.015676\n0.026419\n\"ay.wav\"\n\n\n-94.634745\n111.380402\n49.117843\n-93.450775\n0.026419\n\"ay.wav\"\n\n\n-15.09858\n12.663267\n-24.569898\n-229.305305\n0.026419\n\"ay.wav\"\n\n\n28.328895\n-26.586388\n5.377181\n227.96503\n0.026419\n\"ay.wav\"\n\n\n\n\n\n\n\nWe can use all of these parameters like a single vector describing the vowel measurement.\n\n\nWidening winner_df\nwinner_df\\\n    .select(\n        pl.col(\"^F\\d$\")\n    )\\\n    .with_row_count()\\\n    .melt(\n        id_vars = \"row_nr\"\n    )\\\n    .with_columns(\n        [\n            pl.concat_str([\n                pl.col(\"variable\"),\n                pl.col(\"row_nr\")\n            ],\n            separator = \"_\")\\\n            .alias(\"param\"),\n            pl.lit(1).alias(\"idx\")\n        ]\n    )\\\n    .select(\n        \"idx\",\n        \"param\", \n        \"value\"\n    )\\\n    .pivot(\n        index = \"idx\",\n        columns = \"param\",\n        values = \"value\"\n    )\n\n\n\n\nshape: (1, 21)\n\n\n\nidx\nF1_0\nF1_1\nF1_2\nF1_3\nF1_4\nF2_0\nF2_1\nF2_2\nF2_3\nF2_4\nF3_0\nF3_1\nF3_2\nF3_3\nF3_4\nF4_0\nF4_1\nF4_2\nF4_3\nF4_4\n\n\ni32\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n1\n543.719318\n166.338746\n-94.634745\n-15.09858\n28.328895\n1648.995798\n-551.8309\n111.380402\n12.663267\n-26.586388\n2460.113393\n34.624759\n49.117843\n-24.569898\n5.377181\n3263.261512\n-234.015676\n-93.450775\n-229.305305\n227.96503\n\n\n\n\n\n\n\nThe 0th parameter for each formant is roughly the formant average, with the subsequent parameters capturing its wiggliness.",
    "crumbs": [
      "Home",
      "Development Plan",
      "`new-fave` Approach"
    ]
  },
  {
    "objectID": "dev_plan/new-fave-approach.html#the-fave-step",
    "href": "dev_plan/new-fave-approach.html#the-fave-step",
    "title": "new-fave Approach",
    "section": "The FAVE step",
    "text": "The FAVE step\nAs a demonstration, we’ll use the parameters from each candidate of this single vowel as if they were the concatenated parameters of many tokens.\n\n\nWidening candidates\nall_param = candidates.to_df(\n    which = \"all\",\n    output = \"param\"\n)\ncandidate_wide = all_param\\\n    .select(\n        pl.col(\"candidate\"),\n        pl.col(\"^F\\d$\")\n    )\\\n    .with_columns(\n       pl.first()\\\n        .cum_count()\\\n        .alias(\"row_number\")\\\n        .over(\"candidate\")\\\n        .flatten()\n    )\\\n    .melt(\n        id_vars = [\"candidate\", \"row_number\"]\n    )\\\n    .with_columns(\n        [\n            pl.concat_str([\n                pl.col(\"variable\"),\n                pl.col(\"row_number\")\n            ],\n            separator = \"_\")\\\n            .alias(\"param\")\n        ]\n    )\\\n    .select(\n        \"candidate\",\n        \"param\",\n        \"value\"\n    )\\\n    .pivot(\n        index = \"candidate\",\n        columns = \"param\",\n        values = \"value\"\n    )\n\ncandidate_wide.head(10)\n\n\n\n\nshape: (10, 21)\n\n\n\ncandidate\nF1_0\nF1_1\nF1_2\nF1_3\nF1_4\nF2_0\nF2_1\nF2_2\nF2_3\nF2_4\nF3_0\nF3_1\nF3_2\nF3_3\nF3_4\nF4_0\nF4_1\nF4_2\nF4_3\nF4_4\n\n\ni32\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n1\n411.335104\n108.807541\n-102.036096\n-52.689006\n-25.430442\n783.986401\n198.205354\n-138.877187\n-102.000196\n73.494964\n1488.618269\n-193.049408\n-51.989343\n79.57853\n-4.763601\n1989.115833\n-203.601313\n95.822211\n35.665854\n28.387325\n\n\n2\n429.48413\n136.489588\n-77.329589\n-32.453246\n-19.683157\n817.272539\n233.431206\n-105.974785\n-75.521639\n84.792808\n1507.828685\n-170.774711\n-20.899267\n87.297997\n0.874288\n2047.827676\n-119.536892\n125.976036\n45.429666\n39.163431\n\n\n3\n444.165099\n156.685253\n-68.149797\n-19.184595\n13.005725\n855.027925\n257.016431\n-95.357076\n-65.852872\n100.541967\n1534.882715\n-170.165738\n-3.097089\n91.15517\n-0.872657\n2108.839063\n-65.823412\n109.600672\n29.18736\n55.365782\n\n\n4\n452.357105\n158.30363\n-66.906984\n-5.01387\n24.562925\n888.114834\n256.869706\n-94.437507\n-27.080022\n72.96863\n1555.024677\n-180.721052\n10.271007\n101.208047\n-14.866143\n2142.452221\n-40.099808\n115.361435\n42.188659\n45.770364\n\n\n5\n476.961738\n190.256353\n-58.04869\n0.315637\n19.199771\n927.518399\n252.303627\n-98.909283\n16.138222\n80.325508\n1579.107135\n-166.086558\n19.17018\n112.869772\n-22.738818\n2192.387336\n5.752127\n115.131124\n61.554801\n72.501953\n\n\n6\n484.791847\n187.910398\n-57.704296\n10.886179\n22.726125\n956.184344\n214.214239\n-82.409025\n56.390907\n59.0058\n1597.356957\n-195.937751\n40.255011\n118.086198\n-34.45321\n2200.732854\n2.434181\n144.815491\n110.501014\n81.387384\n\n\n7\n495.036613\n193.349551\n-69.951948\n10.774951\n16.751092\n987.534393\n214.913872\n-107.595259\n79.787332\n15.732351\n1604.040272\n-200.172403\n31.606391\n120.481457\n-49.494178\n2235.731721\n24.121842\n121.358158\n103.39044\n80.487263\n\n\n8\n505.683475\n182.303045\n-73.760086\n14.58709\n19.690183\n1034.505326\n169.858442\n-89.494138\n69.177636\n4.062592\n1618.081158\n-234.396471\n50.928053\n106.48603\n-48.678448\n2266.314438\n35.21776\n117.828152\n92.994988\n78.844444\n\n\n9\n499.951318\n182.999354\n-60.56169\n17.138942\n4.546051\n1039.623508\n190.562471\n-88.527935\n57.876446\n-20.929407\n1628.877756\n-238.979421\n52.885694\n91.89496\n-62.117938\n2297.68883\n75.266141\n112.795521\n52.782001\n28.298095\n\n\n10\n510.927491\n183.141922\n-77.440759\n12.17978\n19.64912\n1081.140063\n190.358285\n-106.550933\n29.734574\n-19.963351\n1650.922389\n-250.762001\n56.052692\n68.602429\n-60.370176\n2348.493902\n118.082061\n85.903483\n-3.35318\n-6.707655\n\n\n\n\n\n\n\nFrom these parameters, we can calculate means and covariance matrices to caclulate the mahalanobis distance of each candidate from the central tendency.\n\nfrom scipy.spatial.distance import mahalanobis\n\n\n\nCalculcating Mahalanobis Distance\nparam_cov = np.cov(\n    candidate_wide\\\n    .select(\n        pl.col(\"^F.*$\"),\n    )\n    .to_numpy()\\\n    .T\n)\n\nparam_inv_cov = np.linalg.inv(param_cov)\n\nparam_means = candidate_wide\\\n    .select(\n        pl.col(\"^F.*$\")\n    )\\\n    .mean()\\\n    .to_numpy()\\\n    .flatten()\n\n\ncandidate_mahal = candidate_wide\\\n    .melt(\n        id_vars = \"candidate\"\n    )\\\n    .group_by(\n        \"candidate\"\n    )\\\n    .agg(\n       pl.col(\"value\")\\\n        .flatten()\\\n        .map_elements(np.array)\\\n        .map_elements(\n            lambda v:\n            mahalanobis(\n                v, \n                param_means,\n                param_inv_cov\n            )\n        )\n    )\\\n    .sort(\n        \"value\"\n    )\n\ncandidate_mahal.head()\n\n\n\n\nshape: (5, 2)\n\n\n\ncandidate\nvalue\n\n\ni32\nf64\n\n\n\n\n78\n2.974149\n\n\n45\n2.982252\n\n\n14\n3.002779\n\n\n44\n3.103581\n\n\n38\n3.177676\n\n\n\n\n\n\n\nIn this particular case, the outcome wasn’t that great, since we were actually searching for the central-most candidate of this candidate set, wheras the actual version would be comparing a candidate set to the smoothest formant tracks from the first pass.\n\ncandidates.winner.spectrogram()\n\n\n\n\n\n\n\n\n\nmahal_idx = candidate_mahal.select(\"candidate\").head(1).item()\ncandidates.candidates[mahal_idx].spectrogram()",
    "crumbs": [
      "Home",
      "Development Plan",
      "`new-fave` Approach"
    ]
  },
  {
    "objectID": "dev_plan/new-fave-approach.html#things-to-consider",
    "href": "dev_plan/new-fave-approach.html#things-to-consider",
    "title": "new-fave Approach",
    "section": "Things to consider",
    "text": "Things to consider\n\nPerhaps the mahalanobis search should be limited to just the first two formants?\nPerhaps there should be a “roughness” cutoff, or the top half of the candidates with the worst mean-squared-error eliminated?",
    "crumbs": [
      "Home",
      "Development Plan",
      "`new-fave` Approach"
    ]
  },
  {
    "objectID": "dev_plan/new-fave-approach.html#other-possibilities",
    "href": "dev_plan/new-fave-approach.html#other-possibilities",
    "title": "new-fave Approach",
    "section": "Other Possibilities",
    "text": "Other Possibilities\nThere may be some possibility of a gradient-descent like approach, perhaps just within candidates.\n\n\nCode\ncandidate_error = candidates.to_df(\n    which = \"all\"\n) \\\n    .group_by(\n        \"candidate\"\n    )\\\n    .agg(\n        pl.col(\"error\").mean(),\n        pl.col(\"max_formant\").mean()\n    )\n\nerror_dfs = candidate_error.join(candidate_mahal, on = \"candidate\")\n\n\n\nfrom matplotlib import pyplot as plt\n\n\n\nCode\nplt.scatter(\n    error_dfs[\"max_formant\"],\n    error_dfs[\"error\"]\n)\nplt.xlabel(\"max formant\")\nplt.ylabel(\"mse\")\n\n\nText(0, 0.5, 'mse')\n\n\n\n\n\n\n\n\n\nThis is roughly the kind of loss function you’d want to see… but its walls aren’t monotonic.\n\n\nCode\nerror_dfs = error_dfs\\\n    .sort(\"max_formant\")\\\n    .with_columns(\n        error_diff = pl.col(\"error\").diff()\n    )\\\n    .with_columns(\n        sign = pl.col(\"error_diff\").sign()\n    )\n\nplt.scatter(\n    error_dfs[\"max_formant\"],\n    error_dfs[\"error_diff\"],\n    c = error_dfs[\"sign\"]\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.scatter(\n    error_dfs[\"max_formant\"],\n    error_dfs[\"value\"]\n)\nplt.xlabel(\"max formant\")\nplt.ylabel(\"mahalanobis\")\n\n\nText(0, 0.5, 'mahalanobis')",
    "crumbs": [
      "Home",
      "Development Plan",
      "`new-fave` Approach"
    ]
  },
  {
    "objectID": "reference/extract.html",
    "href": "reference/extract.html",
    "title": "extract",
    "section": "",
    "text": "extract\n\n\n\n\n\nName\nDescription\n\n\n\n\nextract\nsummary\n\n\n\n\n\nextract.extract()\nsummary",
    "crumbs": [
      "Extract",
      "extract"
    ]
  },
  {
    "objectID": "reference/extract.html#functions",
    "href": "reference/extract.html#functions",
    "title": "extract",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nextract\nsummary\n\n\n\n\n\nextract.extract()\nsummary",
    "crumbs": [
      "Extract",
      "extract"
    ]
  }
]